"""
Bio-Short-Seller Supervisor - LangGraph Orchestration

This module orchestrates all specialized agents using LangGraph.

Workflow:
    User Input
        ‚Üì
    Harvester (BioHarvestEngine)
        ‚Üì
    [Parallel Execution]
        ‚îú‚Üí Miner (EvidenceEngine)
        ‚îî‚Üí Auditor (ForensicEngine)
        ‚Üì
    GraphBuilder (Data Aggregation & Validation)
        ‚Üì
    Writer (ReportWriterAgent)
        ‚Üì
    Final Report
"""

import os
import json
from pathlib import Path
from typing import Dict, Any
from loguru import logger

from langgraph.graph import StateGraph, START, END

from src.graph.state import AgentState
from config import settings
from BioHarvestEngine.agent import BioHarvestAgent
from ForensicEngine.agent import ForensicAuditorAgent
from EvidenceEngine.agent import create_agent as create_evidence_agent
from src.agents.report_writer import create_agent as create_report_agent


# ========== Node Implementations ==========

def harvester_node(state: AgentState) -> Dict[str, Any]:
    """
    Node 1: BioHarvestEngine - Literature & Clinical Trial Harvester
    
    Searches PubMed and ClinicalTrials.gov for relevant papers and trials.
    Extracts PDF links and metadata for downstream analysis.
    
    Args:
        state: Current workflow state with user_query
    
    Returns:
        Updated state with:
            - harvested_data: List of papers/trials
            - pdf_paths: List of PDF URLs/paths (if available)
    """
    logger.info("\n" + "="*80)
    logger.info("üî¨ NODE 1: BIO-HARVEST ENGINE")
    logger.info("="*80)
    
    try:
        agent = BioHarvestAgent()
        
        # Run harvest
        results = agent.run(
            user_query=state["user_query"],
            max_results_per_source=20
        )
        
        # Extract LOCAL PDF paths from downloaded papers
        # BioHarvestEngine now includes 'local_path' field after downloading PMC PDFs
        pdf_paths = []
        for result in results.get("results", []):
            local_path = result.get("local_path")
            if local_path and os.path.exists(local_path):
                pdf_paths.append(local_path)
        
        logger.success(f"‚úÖ Harvested {len(results.get('results', []))} papers/trials")
        logger.info(f"üìÑ Found {len(pdf_paths)} valid local PDFs for analysis")
        
        return {
            "harvested_data": results.get("results", []),
            "pdf_paths": pdf_paths,  # Now contains local file paths, not URLs
            "pdf_files": pdf_paths,  # üî• STEP 3: Store as pdf_files for global tracking
            "status": "harvest_complete"
        }
        
    except Exception as e:
        logger.error(f"‚ùå Harvester failed: {e}")
        return {
            "harvested_data": [],
            "pdf_paths": [],
            "errors": [f"Harvester: {str(e)}"],
            "status": "harvest_failed"
        }


def miner_node(state: AgentState) -> Dict[str, Any]:
    """
    Node 2A: EvidenceEngine - Dark Data Miner (Parallel)
    
    Analyzes PDF supplementary materials for buried negative results.
    Runs in parallel with ForensicEngine.
    
    Args:
        state: Current workflow state with pdf_paths
    
    Returns:
        Updated state with:
            - text_evidence: List of EvidenceItem dicts
            - compiled_evidence_text: Aggregated text content for ReportWriter
            - failed_files: List of files that failed to process
            - total_files: Total number of PDFs attempted
    """
    logger.info("\n" + "="*80)
    logger.info("üïµÔ∏è NODE 2A: EVIDENCE MINER (Parallel)")
    logger.info("="*80)
    
    try:
        agent = create_evidence_agent()
        
        pdf_paths = state.get("pdf_paths", [])
        
        if not pdf_paths:
            logger.warning("‚ö†Ô∏è No PDFs provided for evidence mining")
            return {
                **state,
                "text_evidence": [],
                "compiled_evidence_text": "",
                "failed_files": [],
                "total_files": 0,
                "status": "mining_skipped"
            }
        
        # üö® STEP 2 FIX: Initialize accumulators with explicit naming
        all_evidence = []
        all_evidence_context = []  # üî• RENAMED: Store actual evidence text content
        failed_files = []  # Track failed file processing
        
        # üî• FIXED: Use configurable PDF limit instead of hardcoded [:3]
        max_pdfs = settings.MAX_PDFS_TO_PROCESS if settings.MAX_PDFS_TO_PROCESS > 0 else len(pdf_paths)
        total_files = min(max_pdfs, len(pdf_paths))
        total_available = len(pdf_paths)
        
        logger.info(f"üìä PDF Processing Config: Processing {total_files} out of {total_available} available PDFs")
        
        # Mine evidence from all PDFs (up to configured limit)
        for i, pdf_path in enumerate(pdf_paths[:total_files], 1):
            logger.info(f"\nüìÑ Mining PDF {i}/{total_files} (Total available: {total_available}): {pdf_path}")
            
            try:
                # pdf_path is now a local file path (downloaded by BioHarvestEngine)
                if os.path.exists(pdf_path):
                    # üî• NEW PROTOCOL: mine_evidence now returns Dict with paper_summary and risk_signals
                    result = agent.mine_evidence(pdf_path)
                    
                    # --- üî• DEBUG BLOCK START ---
                    print(f"\nüî• DEBUG: Inspecting Miner Result for PDF {i}")
                    print(f"üî• DEBUG: Result Type: {type(result)}")
                    print(f"üî• DEBUG: Available Keys: {list(result.keys()) if isinstance(result, dict) else 'Not a dict'}")
                    # --- üî• DEBUG BLOCK END ---
                    
                    # ============ ROBUST PROTOCOL: NORMALIZE INPUT ============
                    # 1. Handle Failed/Null Results
                    if result is None:
                        logger.error(f"‚ùå Received None result for {os.path.basename(pdf_path)}")
                        failed_files.append(os.path.basename(pdf_path))
                        continue
                    
                    # 2. Normalize Input (Handle Object vs Dict vs List)
                    # EXTRACT DATA (Handle Dict/Object)
                    data = result.__dict__ if hasattr(result, '__dict__') else result
                    if not isinstance(data, dict): 
                        if isinstance(data, list):
                            # Legacy/Fallback case: It's just a list of risks
                            logger.warning(f"‚ö†Ô∏è Legacy List Format detected")
                            data = {"paper_summary": "Summary not extracted (Legacy List).", "risk_signals": data, "filename": os.path.basename(pdf_path)}
                        else:
                            data = {}
                    
                    # 3. CRITICAL: Extract Summary & Risks with validation
                    # Check multiple keys to be safe
                    filename = data.get('filename', os.path.basename(pdf_path))
                    summary = data.get('paper_summary') or data.get('summary') or data.get('scientific_summary') or "[CRITICAL WARNING: CONTENT MISSING]"
                    risks = data.get('risk_signals') or data.get('risk_flags') or []
                    
                    # üî• NEW: Validate data quality before aggregation
                    # Skip files with errors or insufficient data
                    if summary.startswith("Error:") or "CONTENT MISSING" in summary:
                        logger.error(f"‚ö†Ô∏è Skipping {filename}: Error in summary - {summary[:100]}")
                        failed_files.append(filename)
                        continue
                    
                    # Check minimum content threshold (at least 200 characters)
                    if len(summary) < 200:
                        logger.warning(f"‚ö†Ô∏è {filename} has insufficient summary: {len(summary)} chars")
                        if len(risks) == 0:
                            logger.error(f"‚ö†Ô∏è Skipping {filename}: No risks and short summary")
                            failed_files.append(filename)
                            continue
                    
                    # Check for empty risks (at least warn)
                    if len(risks) == 0:
                        logger.warning(f"‚ö†Ô∏è {filename} has no risk signals detected")
                    
                    # üî• NEW: Track content quality metrics
                    logger.debug(f"‚úÖ {filename}: {len(summary)} chars, {len(risks)} risks")
                    
                    # 4. FORCE CONCATENATION - Build rich context payload
                    entry = f"""
=== EVIDENCE SOURCE: {filename} ===
> **SUMMARY**: {summary}
> **RISK FINDINGS**: {json.dumps(risks, indent=2)}
--------------------------------------------------
"""
                    all_evidence_context.append(entry)
                    
                    # Convert risks to dict format for storage
                    evidence_dicts = []
                    for risk in risks:
                        if hasattr(risk, '__dict__'):
                            evidence_dicts.append({
                                "source": risk.source,
                                "page_estimate": risk.page_estimate,
                                "quote": risk.quote,
                                "risk_level": risk.risk_level,
                                "risk_type": risk.risk_type,
                                "explanation": risk.explanation
                            })
                        else:
                            evidence_dicts.append(risk)
                    
                    all_evidence.extend(evidence_dicts)
                    logger.success(f"‚úÖ Extracted {len(risks)} findings from PDF {i}")
                        
                else:
                    # üö® PHASE 2 FIX: Track missing files as failures
                    logger.error(f"‚ùå PDF not found locally: {pdf_path}")
                    failed_files.append(os.path.basename(pdf_path))
            
            except Exception as e:
                # üö® PHASE 2 FIX: Catch failures and track them explicitly
                logger.error(f"‚ùå Failed to mine PDF {i}: {e}")
                import traceback
                traceback.print_exc()
                failed_files.append(os.path.basename(pdf_path))
                continue
        
        # Calculate success/failure metrics
        success_count = total_files - len(failed_files)
        logger.success(f"‚úÖ Mined {len(all_evidence)} evidence items from {success_count}/{total_files} PDFs")
        
        if failed_files:
            logger.warning(f"‚ö†Ô∏è Failed to process {len(failed_files)} files: {', '.join(failed_files)}")
        
        # üî• ENHANCED: Log and validate payload size
        total_chars = len("".join(all_evidence_context))
        logger.info(f"üî• DEBUG: Final Context Payload: {total_chars} chars (Target: >5000)")
        
        # üî• NEW: Validate minimum content threshold
        MIN_PAYLOAD_SIZE = 1000  # Minimum 1000 chars for meaningful analysis
        if total_chars < MIN_PAYLOAD_SIZE:
            logger.error(f"‚ùå CRITICAL: Payload too small ({total_chars} chars < {MIN_PAYLOAD_SIZE})")
            logger.error(f"‚ùå Failed files: {len(failed_files)}, Successful: {success_count}")
            if success_count == 0:
                logger.error("‚ùå All PDFs failed - cannot generate report")
                # Return error state instead of proceeding
                return {
                    "error": "All PDF extractions failed",
                    "failed_files": failed_files,
                    "total_files": total_files,
                    "context_size": total_chars
                }
        
        logger.success(f"‚úÖ Context payload validated: {total_chars} chars from {success_count} PDFs")
        
        return {
            "text_evidence": all_evidence,
            "compiled_evidence_text": "".join(all_evidence_context),  # üî• STEP 2 FIX: Use new variable
            "failed_files": failed_files,  # üö® NEW: Track failures
            "total_files": total_files,  # üö® NEW: Track total attempted
            "pdf_files": state.get("pdf_paths", [])  # üî• STEP 3: Store global PDF list
        }
        
    except Exception as e:
        logger.error(f"‚ùå Evidence Miner failed: {e}")
        return {
            "text_evidence": [],
            "compiled_evidence_text": "",
            "failed_files": [],
            "total_files": 0,
            "errors": [f"Miner: {str(e)}"]
        }


def auditor_node(state: AgentState) -> Dict[str, Any]:
    """
    Node 2B: ForensicEngine - Image Forensic Auditor (Parallel)
    
    Analyzes scientific figures for potential manipulation.
    Runs in parallel with EvidenceEngine.
    
    Args:
        state: Current workflow state with pdf_paths
    
    Returns:
        Updated state with:
            - forensic_evidence: List of ImageAuditResult dicts
            - forensic_failed_files: List of files that failed forensic audit
    """
    logger.info("\n" + "="*80)
    logger.info("üîç NODE 2B: FORENSIC AUDITOR (Parallel)")
    logger.info("="*80)
    
    try:
        agent = ForensicAuditorAgent()
        
        pdf_paths = state.get("pdf_paths", [])
        
        if not pdf_paths:
            logger.warning("‚ö†Ô∏è No PDFs provided for forensic audit")
            return {
                "forensic_evidence": [],
                "forensic_failed_files": []
            }
        
        # üö® PHASE 2 FIX: Track forensic audit failures
        all_audit_results = []
        forensic_failed_files = []
        
        # üî• FIXED: Use configurable PDF limit instead of hardcoded [:3]
        max_pdfs = settings.MAX_PDFS_TO_PROCESS if settings.MAX_PDFS_TO_PROCESS > 0 else len(pdf_paths)
        total_forensic_files = min(max_pdfs, len(pdf_paths))
        total_available = len(pdf_paths)
        
        logger.info(f"üìä Forensic Audit Config: Processing {total_forensic_files} out of {total_available} available PDFs")
        
        # Audit all PDFs (up to configured limit)
        for i, pdf_path in enumerate(pdf_paths[:total_forensic_files], 1):
            logger.info(f"\nüìÑ Auditing PDF {i}/{total_forensic_files} (Total available: {total_available}): {pdf_path}")
            
            try:
                if os.path.exists(pdf_path):
                    audit_results = agent.audit_paper(pdf_path)
                    
                    # Convert ImageAuditResult dataclasses to dicts
                    audit_dicts = [
                        {
                            "image_id": r.image_id,
                            "image_path": r.image_path,
                            "page_num": r.page_num,
                            "status": r.status,
                            "tampering_risk_score": r.tampering_risk_score,
                            "findings": r.findings,
                            "raw_analysis": r.raw_analysis
                        }
                        for r in audit_results
                    ]
                    
                    all_audit_results.extend(audit_dicts)
                    logger.success(f"‚úÖ Audited {len(audit_dicts)} images from PDF {i}")
                else:
                    # üö® PHASE 2 FIX: Track missing files
                    logger.error(f"‚ùå PDF not found locally: {pdf_path}")
                    forensic_failed_files.append(os.path.basename(pdf_path))
            
            except Exception as e:
                # üö® PHASE 2 FIX: Track forensic audit failures
                logger.error(f"‚ùå Failed to audit PDF {i}: {e}")
                forensic_failed_files.append(os.path.basename(pdf_path))
                continue
        
        success_count = total_forensic_files - len(forensic_failed_files)
        logger.success(f"‚úÖ Audited {len(all_audit_results)} images from {success_count}/{total_forensic_files} PDFs")
        
        if forensic_failed_files:
            logger.warning(f"‚ö†Ô∏è Forensic audit failed for {len(forensic_failed_files)} files: {', '.join(forensic_failed_files)}")
        
        return {
            "forensic_evidence": all_audit_results,
            "forensic_failed_files": forensic_failed_files  # üö® NEW: Track forensic failures
        }
        
    except Exception as e:
        logger.error(f"‚ùå Forensic Auditor failed: {e}")
        return {
            "forensic_evidence": [],
            "forensic_failed_files": [],
            "errors": [f"Auditor: {str(e)}"],
            "status": "audit_failed"
        }


def graph_builder_node(state: AgentState) -> Dict[str, Any]:
    """
    Node 3: Graph Builder - Data Aggregation & Validation
    
    Waits for both Miner and Auditor to complete, then validates
    and prepares data for the Report Writer.
    
    This node acts as a synchronization point and can optionally
    build knowledge graphs or perform additional data structuring.
    
    üö® PHASE 2: Anti-Silent-Failure Logic
    Calculates risk_override based on failed files to prevent
    "toxic positivity" in reports when PDFs fail to process.
    
    Args:
        state: Current workflow state with text_evidence and forensic_evidence
    
    Returns:
        Updated state with validated data and risk_override
    """
    logger.info("\n" + "="*80)
    logger.info("üß© NODE 3: GRAPH BUILDER (Data Aggregation)")
    logger.info("="*80)
    
    try:
        # Validate data presence
        harvested_count = len(state.get("harvested_data", []))
        text_evidence_count = len(state.get("text_evidence", []))
        forensic_count = len(state.get("forensic_evidence", []))
        
        logger.info(f"üìä Data Summary:")
        logger.info(f"   - Harvested Papers/Trials: {harvested_count}")
        logger.info(f"   - Text Evidence Items: {text_evidence_count}")
        logger.info(f"   - Forensic Audit Results: {forensic_count}")
        
        # üö® PHASE 2: Calculate failure metrics
        failed_files = state.get("failed_files", [])
        forensic_failed = state.get("forensic_failed_files", [])
        
        # üìä STEP 3: Use Global PDF Count
        total_files = len(state.get("pdf_files", []))  # Global count from harvester
        if total_files == 0:
            # Fallback to pdf_paths
            total_files = len(state.get("pdf_paths", []))
        if total_files == 0:
            # Last fallback: use total_files from miner
            total_files = state.get("total_files", 0)
        
        # Merge all failed files (deduplicate)
        all_failed_files = list(set(failed_files + forensic_failed))
        total_failed = len(all_failed_files)
        
        logger.info(f"\nüö® Failure Analysis:")
        logger.info(f"   - Total PDFs Attempted: {total_files}")
        logger.info(f"   - Evidence Mining Failed: {len(failed_files)}")
        logger.info(f"   - Forensic Audit Failed: {len(forensic_failed)}")
        logger.info(f"   - Total Unique Failures: {total_failed}")
        
        # üö® PHASE 2: Implement Anti-Silent-Failure Logic
        risk_override = None
        analysis_status = "COMPLETE"
        
        if total_files == 0:
            logger.warning("‚ö†Ô∏è No PDFs were processed")
            risk_override = "UNKNOWN (NO DATA)"
            analysis_status = "NO_DATA"
        elif total_failed == total_files:
            logger.error("‚ùå CRITICAL: All PDFs failed to process!")
            risk_override = "UNKNOWN (CRITICAL DATA FAILURE)"
            analysis_status = "CRITICAL_FAILURE"
        elif total_failed > 0:
            failure_rate = (total_failed / total_files) * 100
            logger.warning(f"‚ö†Ô∏è PARTIAL SUCCESS: {total_failed}/{total_files} files failed ({failure_rate:.0f}%)")
            risk_override = f"UNCERTAIN (INCOMPLETE DATA - {failure_rate:.0f}% failed)"
            analysis_status = "PARTIAL_SUCCESS"
        else:
            logger.success("‚úÖ All PDFs processed successfully")
            analysis_status = "COMPLETE"
        
        # Extract project name from query if not set
        project_name = state.get("project_name")
        if not project_name:
            # Simple extraction: first capitalized word sequence
            words = state["user_query"].split()
            project_name = " ".join(words[:3]) if words else "Unknown"
        
        # Validate minimum data requirements
        if harvested_count == 0:
            logger.warning("‚ö†Ô∏è No harvested data available")
        
        if text_evidence_count == 0 and forensic_count == 0:
            logger.warning("‚ö†Ô∏è No evidence data available (neither text nor forensic)")
        
        logger.success("‚úÖ Data aggregation complete - ready for report generation")
        logger.info(f"   Analysis Status: {analysis_status}")
        if risk_override:
            logger.warning(f"   Risk Override: {risk_override}")
        
        return {
            "project_name": project_name,
            "status": "graph_complete",
            "analysis_status": analysis_status,  # üö® NEW: Track analysis completeness
            "risk_override": risk_override,  # üö® NEW: Override risk assessment if data missing
            "total_failed_files": total_failed,  # üö® NEW: Total failure count
            "all_failed_files": all_failed_files  # üö® NEW: Combined failure list
        }
        
    except Exception as e:
        logger.error(f"‚ùå Graph Builder failed: {e}")
        return {
            "errors": [f"GraphBuilder: {str(e)}"],
            "status": "graph_failed",
            "risk_override": "UNKNOWN (SYSTEM ERROR)"
        }


def writer_node(state: AgentState) -> Dict[str, Any]:
    """
    Node 4: ReportWriter - Final Report Synthesis
    
    Synthesizes all evidence into a comprehensive biomedical due diligence report.
    
    üö® PHASE 2: Passes failure metadata to enforce honest reporting
    
    Args:
        state: Current workflow state with all evidence data
    
    Returns:
        Updated state with:
            - final_report: Generated markdown report
    """
    logger.info("\n" + "="*80)
    logger.info("üìù NODE 4: REPORT WRITER")
    logger.info("="*80)
    
    try:
        agent = create_report_agent()
        
        # Prepare data for report writer
        harvest_data = {
            "query": state["user_query"],
            "results": state.get("harvested_data", [])
        }
        
        forensic_data = state.get("forensic_evidence", [])
        evidence_data = state.get("text_evidence", [])
        
        # üö® PHASE 2: Extract failure metadata for honest reporting
        compiled_evidence_text = state.get("compiled_evidence_text", "")
        
        # üìä STEP 3: Fix Global Statistics Calculation
        # Get Global Count from State (total PDFs from harvester, not just processed batch)
        total_pdfs_global = len(state.get("pdf_files", []))  # Global list from harvester
        if total_pdfs_global == 0:
            # Fallback to pdf_paths if pdf_files not available
            total_pdfs_global = len(state.get("pdf_paths", []))
        if total_pdfs_global == 0:
            # Last fallback: use total_files from miner
            total_pdfs_global = state.get("total_files", 0)
        
        total_failed = state.get("total_failed_files", 0)
        risk_override = state.get("risk_override")
        analysis_status = state.get("analysis_status", "UNKNOWN")
        failed_files_list = state.get("all_failed_files", [])
        
        logger.info(f"\nüö® Passing failure metadata to Report Writer:")
        logger.info(f"   - Analysis Status: {analysis_status}")
        logger.info(f"   - Total Files (GLOBAL): {total_pdfs_global}")  # üî• Clarify it's global
        logger.info(f"   - Failed Files: {total_failed}")
        if risk_override:
            logger.warning(f"   - Risk Override: {risk_override}")
        if failed_files_list:
            logger.warning(f"   - Failed: {', '.join(failed_files_list)}")
        
        # üìä STEP 3: Update Writer Payload with Global Stats
        payload = {
            "user_query": state["user_query"],
            "harvest_data": harvest_data,
            "forensic_data": forensic_data,
            "evidence_data": evidence_data,
            "project_name": state.get("project_name"),
            "output_dir": "final_reports",
            # üö® PHASE 2: Pass failure metadata
            "compiled_evidence_text": compiled_evidence_text,
            "failed_count": total_failed,
            "total_files": total_pdfs_global,  # üî• Dynamic Global Count (not batch size)
            "risk_override": risk_override,
            "analysis_status": analysis_status,
            "failed_files": failed_files_list
        }
        
        print(f"\nüî• DEBUG: Final Payload to Writer:")
        print(f"üî• DEBUG: - evidence_context length: {len(compiled_evidence_text)}")
        print(f"üî• DEBUG: - total_files (global): {total_pdfs_global}")
        print(f"üî• DEBUG: - failed_count: {total_failed}")
        
        # Generate report with failure context
        report_output = agent.write_report(**payload)
        
        logger.success(f"‚úÖ Report generated successfully")
        logger.info(f"   Recommendation: {report_output.recommendation}")
        logger.info(f"   Risk Score: {report_output.risk_score:.1f}/10")
        
        # PHASE 3.1 FIX: Return markdown content AND file path for frontend injection
        return {
            "final_report": report_output.markdown_content,
            "final_report_path": report_output.markdown_path,  # NEW: Add file path
            "final_report_markdown": report_output.markdown_content,
            "recommendation": report_output.recommendation,
            "risk_score": report_output.risk_score,
            "status": "complete"
        }
        
    except Exception as e:
        logger.error(f"‚ùå Report Writer failed: {e}")
        import traceback
        traceback.print_exc()
        
        return {
            "final_report": f"# Report Generation Failed\n\nError: {str(e)}",
            "errors": [f"Writer: {str(e)}"],
            "status": "writer_failed"
        }


# ========== Graph Construction ==========

def create_bio_short_seller_workflow() -> StateGraph:
    """
    Create the Bio-Short-Seller LangGraph workflow.
    
    Workflow Architecture:
        START
          ‚Üì
        Harvester (BioHarvestEngine)
          ‚Üì
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚Üì             ‚Üì
      Miner       Auditor  (Parallel Execution)
    (Evidence)  (Forensic)
        ‚Üì             ‚Üì
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚Üì
        GraphBuilder (Sync & Validate)
          ‚Üì
        Writer (ReportWriter)
          ‚Üì
        END
    
    Returns:
        Compiled LangGraph application
    """
    logger.info("üèóÔ∏è Building Bio-Short-Seller workflow...")
    
    # Initialize state graph
    workflow = StateGraph(AgentState)
    
    # Add nodes
    workflow.add_node("harvester", harvester_node)
    workflow.add_node("miner", miner_node)
    workflow.add_node("auditor", auditor_node)
    workflow.add_node("graph_builder", graph_builder_node)
    workflow.add_node("writer", writer_node)
    
    # Define edges
    # START -> Harvester
    workflow.add_edge(START, "harvester")
    
    # Harvester -> Parallel execution (Miner + Auditor)
    workflow.add_edge("harvester", "miner")
    workflow.add_edge("harvester", "auditor")
    
    # Parallel nodes -> GraphBuilder (synchronization point)
    workflow.add_edge("miner", "graph_builder")
    workflow.add_edge("auditor", "graph_builder")
    
    # GraphBuilder -> Writer
    workflow.add_edge("graph_builder", "writer")
    
    # Writer -> END
    workflow.add_edge("writer", END)
    
    logger.success("‚úÖ Workflow graph constructed")
    
    return workflow


def compile_workflow():
    """
    Compile the Bio-Short-Seller workflow.
    
    Returns:
        Compiled LangGraph application ready for execution
    """
    workflow = create_bio_short_seller_workflow()
    app = workflow.compile()
    
    logger.success("‚úÖ Workflow compiled successfully")
    
    return app


# ========== Workflow Execution ==========

def run_bio_short_seller(user_query: str, pdf_paths: list = None) -> Dict[str, Any]:
    """
    Execute the Bio-Short-Seller workflow end-to-end.
    
    Args:
        user_query: User's research question (e.g., "Analyze CAR-T therapy X safety")
        pdf_paths: Optional list of local PDF paths to analyze
    
    Returns:
        Final state dictionary with all results
    
    Example:
        >>> result = run_bio_short_seller(
        ...     user_query="Investigate pembrolizumab cardiotoxicity",
        ...     pdf_paths=["paper1.pdf", "paper2.pdf"]
        ... )
        >>> print(result["final_report"])
    """
    logger.info("\n" + "üöÄ"*40)
    logger.info("BIO-SHORT-SELLER WORKFLOW INITIATED")
    logger.info("üöÄ"*40 + "\n")
    
    # Compile workflow
    app = compile_workflow()
    
    # Initialize state
    initial_state: AgentState = {
        "user_query": user_query,
        "pdf_paths": pdf_paths or [],
        "harvested_data": [],
        "text_evidence": [],
        "forensic_evidence": [],
        "final_report": None,
        "project_name": None,
        "status": "initialized",
        "errors": []
    }
    
    # Execute workflow
    logger.info(f"üìã Query: {user_query}")
    logger.info(f"üìÑ Pre-loaded PDFs: {len(pdf_paths or [])}\n")
    
    try:
        # Run the workflow
        final_state = app.invoke(initial_state)
        
        logger.info("\n" + "="*80)
        logger.info("‚úÖ WORKFLOW COMPLETE")
        logger.info("="*80)
        
        # Display summary
        logger.info(f"\nüìä Final Summary:")
        logger.info(f"   Status: {final_state.get('status', 'unknown')}")
        logger.info(f"   Harvested Items: {len(final_state.get('harvested_data', []))}")
        logger.info(f"   Text Evidence: {len(final_state.get('text_evidence', []))}")
        logger.info(f"   Forensic Evidence: {len(final_state.get('forensic_evidence', []))}")
        logger.info(f"   Report Generated: {'Yes' if final_state.get('final_report') else 'No'}")
        
        if final_state.get("errors"):
            logger.warning(f"   Errors Encountered: {len(final_state['errors'])}")
            for error in final_state["errors"]:
                logger.warning(f"     - {error}")
        
        return final_state
        
    except Exception as e:
        logger.error(f"\n‚ùå Workflow execution failed: {e}")
        import traceback
        traceback.print_exc()
        raise


if __name__ == "__main__":
    # Test the workflow
    test_query = "Analyze CAR-T therapy safety concerns"
    result = run_bio_short_seller(test_query)
    
    if result.get("final_report"):
        print("\n" + "="*80)
        print("FINAL REPORT (Preview):")
        print("="*80)
        print(result["final_report"][:500] + "...")
